# Import necessary libraries
from sense2vec import Sense2Vec
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from collections import OrderedDict
import spacy
from llama_dist import generate_dist
import re
import numpy as np
from keybert import KeyBERT

# Step 1: Load the Sense2Vec model and spaCy model
s2v = Sense2Vec().from_disk('s2v_old')
nlp = spacy.load("en_core_web_sm")

# Step 2: Define function to get words similar in sense from Sense2Vec
def sense2vec_get_words(word, s2v):
    output = []
    word = word.lower().replace(" ", "_")
    word_parts = word.split('_')  # Break the original word into parts (in case it's multi-word)
    
    # Get the best sense of the word
    sense = s2v.get_best_sense(word)
    print(sense)
    
    if sense is None:
        print(f"No sense found for: {word}")
        return [] 
    
    sense_type = sense.split("|")[1]  # Extract the sense type (e.g., PERSON)
    
    # Retrieve most similar words based on sense
    most_similar = s2v.most_similar(sense, n=20)

    for each_word in most_similar:
        similar_word, similar_sense = each_word[0].split("|")
        append_word = similar_word.replace("_", " ").title()

        # Check if the distractor has the same sense and does not contain parts of the original word
        if similar_sense == sense_type and not any(part in append_word.lower() for part in word_parts):
            output.append(append_word)

    # Remove duplicates
    return list(OrderedDict.fromkeys(output))

def extract_distractors(response):
    """
    Extracts distractors from the response generated by Llama 3.1.
    
    Args:
        response (str): The response text containing distractors.
    
    Returns:
        list: A list of extracted distractor answers.
    """
    # Regular expression to find distractors in the format "1. Distractor"
    distractors = re.findall(r'\d+\.\s(.+)', response)
    return distractors

# Step 3: Function to compute embeddings for both answer and context
def get_answer_and_context_embeddings(answer, context):
    model = SentenceTransformer('all-MiniLM-L12-v2')

    # Get embeddings for the answer and context
    answer_embedding = model.encode([answer])
    context_embedding = model.encode([context])

    return answer_embedding, context_embedding

# Step 4: Implement Maximal Marginal Relevance (MMR) to rank distractors
def mmr(doc_embedding: np.ndarray,
        word_embeddings: np.ndarray,
        words: list,
        top_n: int = 5,
        diversity: float = 0.8) -> list:
    """
    MMR-based ranking of distractors for both relevance and diversity.

    Arguments:
        doc_embedding: Embedding of the original word (answer) and context combined.
        word_embeddings: Embeddings of candidate distractors.
        words: List of distractors.
        top_n: The number of distractors to return.
        diversity: A weight to adjust the balance between relevance and diversity (0 = focus on relevance, 1 = focus on diversity).

    Returns:
        List of selected distractors ranked by MMR or an empty list if no distractors are found.
    """
    if len(words) == 0 or len(word_embeddings) == 0:
        return []

    # Compute similarity between document (answer + context) and distractors
    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)

    # Compute similarity between distractors themselves
    word_similarity = cosine_similarity(word_embeddings)

    # Initialize list for selected distractors
    selected_distractors = [np.argmax(word_doc_similarity)]
    candidates = [i for i in range(len(words)) if i != selected_distractors[0]]

    for _ in range(top_n - 1):
        if len(candidates) == 0:
            break
        
        candidate_similarities = word_doc_similarity[candidates, :]
        target_similarities = np.max(word_similarity[candidates][:, selected_distractors], axis=1)

        mmr = (1 - diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)

        if mmr.size == 0:
            break
        
        mmr_idx = candidates[np.argmax(mmr)]
        selected_distractors.append(mmr_idx)
        candidates.remove(mmr_idx)

    return [words[idx] for idx in selected_distractors]

# Step 5: Updated function to clean keywords from articles and retry
def clean_keyword_for_sense2vec(keyword):
   
    # Split into words and remove common stopwords like 'the', 'a', 'an'
    stopwords = {"the", "a", "an", "of", "in", "for"}
    words = [word for word in keyword.lower().split() if word not in stopwords]
    
    # Join cleaned words
    return " ".join(words)

# Initialize KeyBERT
kw_model = KeyBERT()

# Step 6: Modify NER keyword extraction to handle sentence answers
def identify_keyword_with_ner(answer):
    doc = nlp(answer)
    for ent in doc.ents:
        if ent.root.pos_ == "NOUN" or ent.label_ in ("CARDINAL", "QUANTITY", "GPE", "ORG", "LOC", "PERSON", "NORP", "DATE", "EVENT"):  # Ensure it's a noun entity
            return clean_keyword_for_sense2vec(ent.text)
    return None

def identify_keyword_with_pos(answer):
    doc = nlp(answer)
    for token in doc:
        if token.pos_ == "NUM" or (token.pos_ == "NOUN" and token.dep_ in ("nsubj", "dobj", "pobj")):
            return clean_keyword_for_sense2vec(token.text)
    for token in doc:
        if token.pos_ == "VERB" and token.dep_ in {"ROOT", "xcomp", "advcl"}:
            return clean_keyword_for_sense2vec(token.text)
    return None

def identify_keyword_with_pos2(answer, question=None):
    answer_doc = nlp(answer)
    question_keywords = set()
    
    # Extract relevant keywords from the question using POS or NER tagging
    if question:
        question_doc = nlp(question)
        for token in question_doc:
            if token.pos_ in {"NOUN", "PROPN", "ADJ"}:  # Capture key nouns, proper nouns, and adjectives
                question_keywords.add(token.lemma_.lower())
    
    # Prioritize adjectives that align with the question's context or theme
    for token in answer_doc:
        if token.pos_ == "ADJ" and (token.lemma_.lower() in question_keywords or token.text in question):
            return clean_keyword_for_sense2vec(token.text)
    
    # Next, prioritize NUM and NOUN with dependencies
    for token in answer_doc:
        if token.pos_ == "NUM" or (token.pos_ == "NOUN" and token.dep_ in ("nsubj", "dobj", "pobj")):
            return clean_keyword_for_sense2vec(token.text)
    
    # Finally, fall back on verbs
    for token in answer_doc:
        if token.pos_ == "VERB" and token.dep_ in {"ROOT", "xcomp", "advcl"}:
            return clean_keyword_for_sense2vec(token.text)
    
    return None


def identify_keyword_with_keybert(answer):
    keywords = kw_model.extract_keywords(answer, keyphrase_ngram_range=(1, 1), stop_words='english')
    if keywords:
        return clean_keyword_for_sense2vec(keywords[0][0])
    return None

def extract_keyword_combined(answer):
    # First try NER
    keyword = identify_keyword_with_ner(answer)
    if keyword and sense2vec_get_words(keyword, s2v):
        return keyword
    else:
        print("not with NER")

    # If NER fails, try POS tagging
    keyword = identify_keyword_with_pos(answer)
    if keyword and sense2vec_get_words(keyword, s2v):
        return keyword
    else:
        print("not with POS")

    # If POS tagging also fails, try KeyBERT
    keyword = identify_keyword_with_keybert(answer)
    if keyword and sense2vec_get_words(keyword, s2v):
        return keyword
    else:
        print("not with keybert")

    # If all methods fail, return None
    return None

# Function to replace the keyword in the sentence with distractors
def replace_keyword_in_sentence(answer, keyword, distractors):
    """
    Replace the keyword in the answer (sentence) with each distractor and return the new sentences.
    """
    variations = []
    for distractor in distractors:
        # Replace only the first occurrence of the keyword
        answer = answer.lower()
        new_sentence = answer.replace(keyword, distractor)
        new_sentence = new_sentence.capitalize()
        variations.append(new_sentence)
    return variations


# Main function to get ranked distractors using MMR
def get_ranked_distractors_mmr(answer, context, s2v, top_n=5, diversity=0.8):
    # Try to get distractors using the original answer directly
    keyword = None
    print(f"Trying to generate distractors for the answer: {answer}")
    distractors = sense2vec_get_words(answer, s2v)

     # If no distractors found, check if the answer is a named entity using NER
    doc = nlp(answer)
    is_named_entity = any(ent.label_ in ["PERSON", "GPE"] for ent in doc.ents)
    
    # If no distractors found, extract a keyword using NER and try again
    if not distractors:
        print(f"No distractors found for the original answer: {answer}")
        
        # Extract keyword using NER
        keyword = extract_keyword_combined(answer)
        if not keyword:
            print("No keyword could be identified. Going to llama")
            llama_response = generate_dist(answer, context)
            distractors = extract_distractors(llama_response)

            if not distractors:
                print("Llama has also failed to give generators")
            # return []
        else:
            print(f"Extracted keyword using Combined approach: {keyword}")
        
        # Get distractors for the keyword
            distractors = sense2vec_get_words(keyword, s2v)
        
        # If still no distractors, return an empty list
            if not distractors:
                print(f"No distractors found for the extracted keyword: {keyword}. Using llama")
            
                llama_response = generate_dist(answer, context)
                distractors = extract_distractors(llama_response)

                if not distractors:
                    print("Llama failed")
                    return []

    
    # Include the original answer/keyword in the distractor list
    distractors.insert(0, answer.title())  # or keyword.title() based on where we found the distractor
    
    # Get embeddings for both the answer and context
    answer_embedding, context_embedding = get_answer_and_context_embeddings(answer, context)
    
    # Combine answer and context embeddings
    doc_embedding = (0.3 * answer_embedding) + (0.7 * context_embedding)

    # Get embeddings for the distractors
    model = SentenceTransformer('all-MiniLM-L12-v2')
    distractor_embeddings = model.encode(distractors)

    # Rank the distractors using MMR
    ranked_distractors = mmr(doc_embedding, distractor_embeddings, distractors, top_n=top_n, diversity=diversity)
    
    # Ensure the original word is not in the final ranked distractors
    ranked_distractors = [d for d in ranked_distractors if d.lower() != answer.lower()]

    # Handle case where the answer is a sentence: replace keyword with distractors
    if len(answer.split()) >= 2 and not is_named_entity:  # This indicates it's likely a sentence answer
        if keyword:
            print(f"Replacing keyword '{keyword}' in the sentence with distractors.")
            # print(keyword)
            print(ranked_distractors)
            replaced_sentences = replace_keyword_in_sentence(answer, keyword, ranked_distractors)
            return replaced_sentences
        else:
            return ranked_distractors
    elif len(answer.split()) >= 2 and is_named_entity:
            return ranked_distractors
    else:
        # print(ranked_distractors)
        print(answer)
        return ranked_distractors
    # print(ranked_distractors)
    # replaced_sentences = replace_keyword_in_sentence(answer, keyword, ranked_distractors)
    # return replaced_sentences

        
    

# Example usage:
# context_sentence = """Laptops have become an essential tool in modern society, providing the flexibility to work from anywhere. They combine the power of a desktop computer with the portability of a mobile device. Modern laptops are equipped with advanced processors, high-resolution displays, and extensive connectivity options. They are used in various fields, including education, business, and entertainment. With the advent of new technologies, laptops have seen significant improvements in battery life, performance, and design. The availability of lightweight models with long battery life makes them ideal for students and professionals on the go."""
# original_word = "Their portability allows users to work from anywhere"
# original_word = "a single dimension of time"
# context_sentence = "The theory of special relativity finds a convenient formulation in Minkowski spacetime, a mathematical structure that combines three dimensions of space with a single dimension of time. In this formalism, distances in space can be measured by how long light takes to travel that distance, e.g., a light-year is a measure of distance, and a meter is now defined in terms of how far light travels in a certain amount of time. Two events in Minkowski spacetime are separated by an invariant interval, which can be either space-like, light-like, or time-like. Events that have a time-like separation cannot be simultaneous in any frame of reference, there must be a temporal component (and possibly a spatial one) to their separation. Events that have a space-like separation will be simultaneous in some frame of reference, and there is no frame of reference in which they do not have a spatial separation. Different observers may calculate different distances and different time intervals between two events, but the invariant interval between the events is independent of the observer (and his velocity)."
# original_word = "Barack Obama"
# context_sentence = "Barack Obama was the 44th president of the United States."
# original_word = "Orienting a building to the Sun"
# context_sentence = """It is an important source of renewable energy and its technologies are broadly characterized as either passive solar or active solar depending on the way they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air."""
# original_word = "24 hymns"
# context_sentence = """Luther's hymns were included in early Lutheran hymnals and spread the ideas of the Reformation. He supplied four of eight songs of the First Lutheran hymnal Achtliederbuch, 18 of 26 songs of the Erfurt Enchiridion, and 24 of the 32 songs in the first choral hymnal with settings by Johann Walter, Eyn geystlich Gesangk Buchleyn, all published in 1524."""
# original_word = "Oil reserves"
# context_sentence = """Tehran is the country's capital and largest city, as well as its leading cultural and economic center. Iran is a major regional and middle power, exerting considerable influence in international energy security and the world economy through its large reserves of fossil fuels, which include the largest natural gas supply in the world and the fourth-largest proven oil reserves. Iran's rich cultural legacy is reflected in part by its 19 UNESCO World Heritage Sites, the fourth-largest number in Asia and 12th-largest in the world."""
# original_word = "the 15th century"
# context_sentence = """It is in this time that the notation of music on a staff and other elements of musical notation began to take shape. This invention made possible the separation of the composition of a piece of music from its transmission; without written music, transmission was oral, and subject to change every time it was transmitted. With a musical score, a work of music could be performed without the composer's presence. The invention of the movable-type printing press in the 15th century had far-reaching consequences on the preservation and transmission of music."""
# original_word = "beasts of burden"
# context_sentence = """In the early Sumerian Uruk period, the primitive pictograms suggest that sheep, goats, cattle, and pigs were domesticated. They used oxen as their primary beasts of burden and donkeys or equids as their primary transport animal and "woollen clothing as well as rugs were made from the wool or hair of the animals. ... By the side of the house was an enclosed garden planted with trees and other plants; wheat and probably other cereals were sown in the fields, and the shaduf was already employed for the purpose of irrigation. Plants were also grown in pots or vases"."""
# Step 6: Get distractors and rank them using MMR
# ranked_distractors = get_ranked_distractors_mmr(original_word, context_sentence, s2v, top_n=5, diversity=0.8)

# # Step 7: Display the results
# if ranked_distractors:
#     print(f"Original Answer: {original_word}")
#     print("Ranked Distractors using MMR (Answer + Context, with Diversity):")
#     for distractor in ranked_distractors:
#         print(distractor)
# else:
#     print(f"No suitable distractors found for the word: {original_word}")
